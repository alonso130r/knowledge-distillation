{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from exllamav2 import ExLlamaV2, ExLlamaV2Config, ExLlamaV2Tokenizer, ExLlamaV2Cache_Q6\n",
    "# from exllamav2 import ExLlamaV2_HF\n",
    "# from transformers import AutoTokenizer\n",
    "import torch\n",
    "from safetensors.torch import save_file\n",
    "from datasets import load_dataset\n",
    "from typing import List\n",
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)\n",
    "num_gpus = torch.cuda.device_count()\n",
    "# devices = [torch.device(\"cuda:0\"), torch.device(\"cuda:1\"), torch.device(\"cuda:2\"), torch.device(\"cuda:3\")]\n",
    "devices = [\"cuda:0\", \"cuda:1\", \"cuda:2\", \"cuda:3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0 can access GPU 1: True\n",
      "GPU 0 can access GPU 2: True\n",
      "GPU 0 can access GPU 3: True\n",
      "GPU 1 can access GPU 0: True\n",
      "GPU 1 can access GPU 2: True\n",
      "GPU 1 can access GPU 3: True\n",
      "GPU 2 can access GPU 0: True\n",
      "GPU 2 can access GPU 1: True\n",
      "GPU 2 can access GPU 3: True\n",
      "GPU 3 can access GPU 0: True\n",
      "GPU 3 can access GPU 1: True\n",
      "GPU 3 can access GPU 2: True\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_gpus):\n",
    "        for j in range(num_gpus):\n",
    "            if i != j:\n",
    "                can_access = torch.cuda.can_device_access_peer(i, j)\n",
    "                print(f\"GPU {i} can access GPU {j}: {can_access}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ExLlamaV2Config()\n",
    "config.model_dir = \"./llama-405b/models--ek826--Meta-Llama-3.1-405B-Instruct-6.0bpw-exl2/snapshots/c3aafe6600c3c081f514e33ea325da7a19cb1822\"\n",
    "config.prepare()\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"./llama-405b/models--ek826--Meta-Llama-3.1-405B-Instruct-6.0bpw-exl2/snapshots/c3aafe6600c3c081f514e33ea325da7a19cb1822\")\n",
    "model = ExLlamaV2(config)\n",
    "tokenizer = ExLlamaV2Tokenizer(config)\n",
    "cache = ExLlamaV2Cache_Q6(model, lazy = True)\n",
    "model.load_autosplit(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "prompt = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
    "questions = ds[\"question\"]\n",
    "questions = questions[:2638]\n",
    "prompt = \"As a teacher, guide your student through solving the question below. Provide a clear, simple explanation for someone unfamiliar with the problem. \"\n",
    "questions = [prompt + question for question in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As a teacher, guide your student through solving the question below. Provide a clear, simple explanation for someone unfamiliar with the problem. Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(inputs : str) -> torch.tensor:\n",
    "    inputs = tokenizer.encode(inputs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.forward(inputs)\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_logits(logits : List[torch.tensor], ) -> None:\n",
    "    data = {}\n",
    "    for i, logit in enumerate(logits):\n",
    "        data[f\"Question {i+1}\"] = logit.to('cpu')\n",
    "\n",
    "    save_file(data, \"llama-3.1-405b-gsm8k-teacher-prompt-tensors.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be used later on to load back the logits\n",
    "\n",
    "from safetensors.torch import safe_open\n",
    "\n",
    "def load_list_of_logits_safetensor(file_path):\n",
    "    # Open the safetensor file\n",
    "    with safe_open(file_path, framework=\"pt\") as f:\n",
    "        logits_list = []\n",
    "        for key in f.keys():\n",
    "            logits_list.append(f.get_tensor(key))\n",
    "    \n",
    "    return logits_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit1 = get_logits(questions[0]).to('cpu')\n",
    "save_logits(logit1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ae006bc9694703848aadc5a3b66fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2638 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logits = [get_logits(question).to('cpu') for question in tqdm(questions)]\n",
    "save_logits(logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
