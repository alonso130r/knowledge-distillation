# knowledge-distillation
Research platform for testing whether knowledge distillation can increase the efficiency and accessibility of locally run LLMs


Target 1: Optimization algorithm to increase the effectiveness/speed of learning