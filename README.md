# knowledge-distillation
Research platform for testing whether knowledge distillation can increase the efficiency and accessibility of locally run LLMs

student: GPT2-Large
teacher: GPT4o

Target 1: Optimization algorithm to increase the effectiveness/speed of learning