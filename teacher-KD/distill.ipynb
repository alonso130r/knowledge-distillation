{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from safetensors.torch import safe_open\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from safetensors.torch import save_file\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import optuna\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 2\n",
    "batch_size = 8\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e823cc8841e4a3990c27b90bf27547a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"../Meta-Llama-3.1-8B\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"../Meta-Llama-3.1-8B\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../Meta-Llama-3.1-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"../Meta-Llama-3.1-8B-Instruct\", torch_dtype=torch.bfloat16, device_map=device) # VERY IMPORTANT: ENSURE USAGE OF BF16 ON ALL TRAINING TASKS TO REDUCE VRAM USAGE\n",
    "tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=4,  # Rank of the low-rank matrix\n",
    "    lora_alpha=8,  # Scaling factor for the LoRA updates\n",
    "    lora_dropout=0.1,  # Dropout to apply after LoRA\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # The modules you want to apply LoRA to\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"openai/gsm8k\", \"socratic\", split=\"train\")\n",
    "data = data.select(range(2638))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = load_dataset(\"openai/gsm8k\", \"socratic\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_list_of_logits_safetensor(file_path):\n",
    "    with safe_open(file_path, framework=\"pt\") as f:\n",
    "        logits_list = []\n",
    "        for key in f.keys():\n",
    "            logits_list.append(f.get_tensor(key))\n",
    "    \n",
    "    return logits_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeDistillationLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Object used to calculate KD loss using a mix of hard loss (cross-entropy) and soft loss (KL-Divergence).\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature=1.0, alpha=0.5):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - temperature (float): Temperature for softening logits before KL-Divergence.\n",
    "        - alpha (float): Weight for combining hard and soft losses.\n",
    "        \"\"\"\n",
    "        super(KnowledgeDistillationLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, student_logits, teacher_logits, labels):\n",
    "        # Hard Loss: Cross-Entropy between student predictions and true labels\n",
    "        loss_hard = self.criterion(student_logits, labels)\n",
    "\n",
    "        # Soft Loss: Reverse KL-Divergence between soft targets from teacher and student\n",
    "        teacher_log_probs = F.log_softmax(teacher_logits / self.temperature, dim=1)\n",
    "        student_probs = F.softmax(student_logits / self.temperature, dim=1)\n",
    "\n",
    "        # NOTE: swap the probs that get log-softmaxed to be the first one passed if switching from KL-Divergence to Reverse KL-Divergence and vice-versa\n",
    "        loss_soft = F.kl_div(teacher_log_probs, student_probs, reduction='batchmean', log_target=False) * (self.temperature ** 2)\n",
    "\n",
    "        # Combine the losses\n",
    "        loss = self.alpha * loss_hard + (1.0 - self.alpha) * loss_soft\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, validation_data, tokenizer, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        # Wrap the iteration over the validation data with tqdm\n",
    "        for example in tqdm(validation_data, desc=\"Evaluating\", unit=\"batch\"):\n",
    "            # Tokenize the input (question) and label (answer)\n",
    "            inputs = tokenizer(example['question'], truncation=True, max_length=256, return_tensors=\"pt\").to(device)\n",
    "            labels = tokenizer(example['answer'], truncation=True, max_length=256, return_tensors=\"pt\")['input_ids'].to(device)\n",
    "\n",
    "            # Forward pass through the model\n",
    "            outputs = model(**inputs)\n",
    "            student_logits = outputs.logits  # Shape [batch_size, sequence_length, vocab_size]\n",
    "\n",
    "            # Adjust sequence lengths to match\n",
    "            seq_len = min(student_logits.size(1), labels.size(1))\n",
    "            student_logits = student_logits[:, :seq_len, :]\n",
    "            labels = labels[:, :seq_len]\n",
    "\n",
    "            # Flatten logits and labels for loss computation\n",
    "            student_logits = student_logits.view(-1, student_logits.size(-1))  # Shape [total_tokens, vocab_size]\n",
    "            labels = labels.view(-1)  # Shape [total_tokens]\n",
    "\n",
    "            # Compute the loss (CrossEntropyLoss in this case)\n",
    "            loss = F.cross_entropy(student_logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    # Return the average loss over the validation set\n",
    "    return total_loss / len(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_logits_L = load_list_of_logits_safetensor('../llama-3.1-405b-gsm8k-teacher-prompt-tensors.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "lora_params = []\n",
    "base_params = []\n",
    "\n",
    "for n, p in model.named_parameters():\n",
    "    if \"lora\" in n:\n",
    "        lora_params.append(p)\n",
    "    else:\n",
    "        base_params.append(p)\n",
    "\n",
    "# Create parameter groups\n",
    "optimizer_grouped_parameters = [\n",
    "    {\"params\": base_params, \"weight_decay\": 0.0},  # No weight decay for base model params\n",
    "    {\"params\": lora_params, \"weight_decay\": 1e-2},  # Apply weight decay to LoRA params\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(optimizer_grouped_parameters, lr=5e-6, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(teacher_logits_L))\n",
    "kd_loss = KnowledgeDistillationLoss(temperature=5.942267335064758, alpha=0.6093348343631224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, teacher_logits_L, data, tokenizer, optimizer, scheduler, kd_loss, num_epochs, device):\n",
    "    \"\"\"\n",
    "    Trains the model using the provided Knowledge Distillation loss.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The student model to train.\n",
    "    - teacher_logits_L: Precomputed teacher logits for the dataset.\n",
    "    - data: The dataset containing questions and answers.\n",
    "    - tokenizer: Tokenizer for the model.\n",
    "    - optimizer: The optimizer for the model.\n",
    "    - scheduler: Learning rate scheduler.\n",
    "    - kd_loss: The Knowledge Distillation loss function.\n",
    "    - num_epochs: Number of epochs to train.\n",
    "    - device: The device (CPU or GPU) to use for training.\n",
    "\n",
    "    Returns:\n",
    "    - Trained model.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Use tqdm to create a progress bar for the entire dataset\n",
    "        progress_bar = tqdm(range(len(data)), desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "        total_loss = 0.0\n",
    "        iterat = 0\n",
    "\n",
    "        for batch_idx in progress_bar:\n",
    "            # Get the current example\n",
    "            example = data[batch_idx]\n",
    "\n",
    "            # Tokenize the input and label on the fly\n",
    "            inputs = tokenizer(example['question'], truncation=True, max_length=256, return_tensors=\"pt\").to(device)\n",
    "            labels = tokenizer(example['answer'], truncation=True, max_length=256, return_tensors=\"pt\")['input_ids'].to(device)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Forward pass for student model\n",
    "            outputs = model(**inputs)\n",
    "            student_logits = outputs.logits  # Shape should be [batch_size, sequence_length, vocab_size]\n",
    "\n",
    "            # Fetch corresponding teacher logits for this batch\n",
    "            teacher_logits = teacher_logits_L[batch_idx].to(device)\n",
    "\n",
    "            # Ensure logits and labels have matching sequence lengths\n",
    "            seq_len = min(student_logits.size(1), labels.size(1), teacher_logits.size(1))\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            student_logits = student_logits[:, :seq_len, :]\n",
    "            labels = labels[:, :seq_len]\n",
    "            teacher_logits = teacher_logits[:, :seq_len, :]\n",
    "\n",
    "            # Flatten logits and labels for loss computation\n",
    "            student_logits = student_logits.view(-1, student_logits.size(-1))  # Shape [batch_size * sequence_length, vocab_size]\n",
    "            labels = labels.view(-1)  # Shape [batch_size * sequence_length]\n",
    "            teacher_logits = teacher_logits.view(-1, student_logits.size(-1))  # Shape [batch_size * sequence_length, vocab_size]\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Compute the KD loss\n",
    "            loss = kd_loss(student_logits, teacher_logits, labels)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Step the scheduler\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Update the progress bar with the current loss\n",
    "            total_loss += loss.item()\n",
    "            iterat += 1\n",
    "            progress_bar.set_postfix(loss=total_loss/iterat)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa8d8920eba4fc2bb02f1cbd681f518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/2:   0%|          | 0/2638 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f208bdc22ce64f15b89a899210503c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/2:   0%|          | 0/2638 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac40a5eec46b42bcbd85731cb1ff27fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1319 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.401957280894317\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, teacher_logits_L, data, tokenizer, optimizer, scheduler, kd_loss, num_epochs, device)\n",
    "val = evaluate_model(model, val_data, tokenizer, device)\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save an initial copy of the model's state_dict\n",
    "initial_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "def objective(trial):\n",
    "    # Reset the model to its initial state\n",
    "    model.load_state_dict(initial_model_state)\n",
    "\n",
    "    # Suggest values for alpha and temperature\n",
    "    alpha = trial.suggest_float(\"alpha\", 0.0, 1.0)\n",
    "    temperature = 5.942267335064758\n",
    "\n",
    "    # Create the Knowledge Distillation loss function with the suggested parameters\n",
    "    kd_loss_fn = KnowledgeDistillationLoss(temperature=temperature, alpha=alpha).to(device)\n",
    "\n",
    "    # Reinitialize the optimizer and scheduler\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=5e-6, momentum=0.9, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(teacher_logits_L))\n",
    "\n",
    "    # Train the model using the `train_model` function\n",
    "    trained_model = train_model(\n",
    "        model=model,\n",
    "        teacher_logits_L=teacher_logits_L,\n",
    "        data=data,\n",
    "        tokenizer=tokenizer,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        kd_loss=kd_loss_fn,\n",
    "        num_epochs=num_epochs,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    val_loss = evaluate_model(trained_model, val_data, tokenizer, device)\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "# Define the number of trials\n",
    "n_trials = 35\n",
    "\n",
    "# Create the Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "# Add a progress bar for the study\n",
    "with tqdm(total=n_trials, desc=\"Temp/Alpha Trials\") as pbar:\n",
    "    def update_pbar(study, trial):\n",
    "        pbar.update(1)\n",
    "    \n",
    "    study.optimize(objective, n_trials=n_trials, callbacks=[update_pbar])\n",
    "\n",
    "with open(\"output2.txt\", \"w\") as file:\n",
    "    file.write(f\"Best alpha with temperature of {5.942267335064758}: {study.best_params['alpha']}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"For 1 ep, Best alpha: {study.best_params['alpha']}, Best temperature: {study.best_params['temperature']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>A new program had 60 downloads in the first month. The number of downloads in the second month was three times as many as the downloads in the first month, but then reduced by 30% in the third month. How many downloads did the program have total over the three months? 3/5/2024 1:00:00 AM\\n## Step 1: Calculate the number of downloads in the first month.\\nThe number of downloads in the first month is given as 60.\\n\\n## Step 2: Calculate the number of downloads in the second month.\\nThe number of downloads in the second month is three times the number of downloads in the first month. So, it is 3 * 60 = 180 downloads.\\n\\n## Step 3: Calculate the number of downloads in the third month.\\nThe number of downloads in the third month is 30% less than the downloads in the second month. So, the number of downloads in the third month is 180 - (30/100) * 180 = 180 - 54 = 126 downloads.\\n\\n## Step 4: Calculate the total number of downloads over the three months.\\nThe total number of downloads is the sum of the downloads in each month. So, the total number of downloads = 60 + 180 + 126 = 366.\\n\\nThe final answer is: $\\\\boxed{366}$ 3/5/2024 1:00:00 AM 3/5/2024 1:00:00 AM 3/5/2024 1:00:00 AM 3/5/2024 1:00:00 AM 3/5/2024 1:00:00 AM 3/5/2024 1:00:00 AM 3/5/2024 1:00:00 AM 3/5/2024 1:00:00 AM 3/5/2024 1:00:00 AM 3/5/2024 1:00:00 AM 3/5/2024 1:00:00 AM 3/5/2024 1:00:00 AM 3/5/2024 1']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"A new program had 60 downloads in the first month. The number of downloads in the second month was three times as many as the downloads in the first month, but then reduced by 30% in the third month. How many downloads did the program have total over the three months?\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=400, do_sample=True, top_p=0.9).to(device)\n",
    "output_answer = tokenizer.batch_decode(outputs)\n",
    "output_answer\n",
    "# with open(\"quadratic.txt\", \"w\") as file:\n",
    "#     file.write(output_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('llama8b-LoRA-IS-2', max_shard_size=\"5GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
