{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from safetensors.torch import safe_open\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from safetensors.torch import save_file\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import optuna\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 1\n",
    "batch_size = 8\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"../Meta-Llama-3.1-8B\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"../Meta-Llama-3.1-8B\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../Meta-Llama-3.1-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"../Meta-Llama-3.1-8B-Instruct\").to(device)\n",
    "model.to(torch.bfloat16) # VERY IMPORTANT: ENSURE USAGE OF BF16 ON ALL TRAINING TASKS TO REDUCE VRAM USAGE\n",
    "student_tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "student_base_model.generation_config.pad_token_id = student_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=4,  # Rank of the low-rank matrix\n",
    "    lora_alpha=8,  # Scaling factor for the LoRA updates\n",
    "    lora_dropout=0.2,  # Dropout to apply after LoRA\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # The modules you want to apply LoRA to\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"openai/gsm8k\", \"socratic\", split=\"train\")\n",
    "# data = data[:2638]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = load_dataset(\"openai/gsm8k\", \"socratic\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_list_of_logits_safetensor(file_path):\n",
    "    with safe_open(file_path, framework=\"pt\") as f:\n",
    "        logits_list = []\n",
    "        for key in f.keys():\n",
    "            logits_list.append(f.get_tensor(key))\n",
    "    \n",
    "    return logits_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeDistillationLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Object used to calculate KD loss using a mix of hard loss (cross-entropy) and soft loss (KL-Divergence).\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature=1.0, alpha=0.5):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - temperature (float): Temperature for softening logits before KL-Divergence.\n",
    "        - alpha (float): Weight for combining hard and soft losses.\n",
    "        \"\"\"\n",
    "        super(KnowledgeDistillationLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, student_logits, teacher_logits, labels):\n",
    "        # Hard Loss: Cross-Entropy between student predictions and true labels\n",
    "        loss_hard = self.criterion(student_logits, labels)\n",
    "\n",
    "        # Soft Loss: Reverse KL-Divergence between soft targets from teacher and student\n",
    "        teacher_log_probs = F.log_softmax(teacher_logits / self.temperature, dim=1)\n",
    "        student_probs = F.softmax(student_logits / self.temperature, dim=1)\n",
    "\n",
    "        # NOTE: swap the probs that get log-softmaxed to be the first one passed if switching from KL-Divergence to Reverse KL-Divergence and vice-versa\n",
    "        loss_soft = F.kl_div(teacher_log_probs, student_probs, reduction='batchmean', log_target=False) * (self.temperature ** 2)\n",
    "\n",
    "        # Combine the losses\n",
    "        loss = self.alpha * loss_hard + (1.0 - self.alpha) * loss_soft\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, validation_data, tokenizer, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for example in validation_data:\n",
    "            # Tokenize the input (question) and label (answer)\n",
    "            inputs = tokenizer(example['question'], truncation=True, max_length=256, return_tensors=\"pt\").to(device)\n",
    "            labels = tokenizer(example['answer'], truncation=True, max_length=256, return_tensors=\"pt\")['input_ids'].to(device)\n",
    "\n",
    "            # Forward pass through the model\n",
    "            outputs = model(**inputs)\n",
    "            student_logits = outputs.logits  # Shape [batch_size, sequence_length, vocab_size]\n",
    "\n",
    "            # Adjust sequence lengths to match\n",
    "            seq_len = min(student_logits.size(1), labels.size(1))\n",
    "            student_logits = student_logits[:, :seq_len, :]\n",
    "            labels = labels[:, :seq_len]\n",
    "\n",
    "            # Flatten logits and labels for loss computation\n",
    "            student_logits = student_logits.view(-1, student_logits.size(-1))  # Shape [total_tokens, vocab_size]\n",
    "            labels = labels.view(-1)  # Shape [total_tokens]\n",
    "\n",
    "            # Compute the loss (CrossEntropyLoss in this case)\n",
    "            loss = F.cross_entropy(student_logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    # Return the average loss over the validation set\n",
    "    return total_loss / len(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_logits_L = load_list_of_logits_safetensor('../llama-3.1-405b-gsm8k-base-tensors.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "lora_params = []\n",
    "base_params = []\n",
    "\n",
    "for n, p in model.named_parameters():\n",
    "    if \"lora\" in n:\n",
    "        lora_params.append(p)\n",
    "    else:\n",
    "        base_params.append(p)\n",
    "\n",
    "# Create parameter groups\n",
    "optimizer_grouped_parameters = [\n",
    "    {\"params\": base_params, \"weight_decay\": 0.0},  # No weight decay for base model params\n",
    "    {\"params\": lora_params, \"weight_decay\": 1e-2},  # Apply weight decay to LoRA params\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(optimizer_grouped_parameters, lr=5e-6, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(teacher_logits_L))\n",
    "kd_loss = KnowledgeDistillationLoss(temperature=5.942267335064758, alpha=0.6093348343631224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, teacher_logits_L, data, tokenizer, optimizer, scheduler, kd_loss, num_epochs, device, batch_size=1):\n",
    "    \"\"\"\n",
    "    Trains the model using the provided Knowledge Distillation loss with manual batch processing.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The student model to train.\n",
    "    - teacher_logits_L: Precomputed teacher logits for the dataset.\n",
    "    - data: The dataset containing questions and answers.\n",
    "    - tokenizer: Tokenizer for the model.\n",
    "    - optimizer: The optimizer for the model.\n",
    "    - scheduler: Learning rate scheduler.\n",
    "    - kd_loss: The Knowledge Distillation loss function.\n",
    "    - num_epochs: Number of epochs to train.\n",
    "    - device: The device (CPU or GPU) to use for training.\n",
    "    - batch_size: Number of samples per batch.\n",
    "\n",
    "    Returns:\n",
    "    - Trained model.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    num_batches = len(data) // batch_size + int(len(data) % batch_size != 0)  # Calculate number of batches\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Use tqdm to create a progress bar for the entire dataset\n",
    "        progress_bar = tqdm(range(num_batches), desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "\n",
    "        for batch_idx in progress_bar:\n",
    "            # Determine the start and end indices for this batch\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(start_idx + batch_size, len(data))\n",
    "\n",
    "            # Get the current batch of examples\n",
    "            batch = data[start_idx:end_idx]\n",
    "            questions = [example['question'] for example in batch]\n",
    "            answers = [example['answer'] for example in batch]\n",
    "\n",
    "            # Tokenize the input and label on the fly\n",
    "            inputs = tokenizer(questions, truncation=True, padding=True, max_length=256, return_tensors=\"pt\").to(device)\n",
    "            labels = tokenizer(answers, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")['input_ids'].to(device)\n",
    "\n",
    "            # Forward pass for student model\n",
    "            outputs = model(**inputs)\n",
    "            student_logits = outputs.logits  # Shape should be [batch_size, sequence_length, vocab_size]\n",
    "\n",
    "            # Fetch corresponding teacher logits for this batch\n",
    "            batch_teacher_logits = teacher_logits_L[start_idx:end_idx].to(device)\n",
    "\n",
    "            # Ensure logits and labels have matching sequence lengths\n",
    "            seq_len = min(student_logits.size(1), labels.size(1), batch_teacher_logits.size(1))\n",
    "\n",
    "            student_logits = student_logits[:, :seq_len, :]\n",
    "            labels = labels[:, :seq_len]\n",
    "            batch_teacher_logits = batch_teacher_logits[:, :seq_len, :]\n",
    "\n",
    "            # Flatten logits and labels for loss computation\n",
    "            student_logits = student_logits.view(-1, student_logits.size(-1))  # Shape [batch_size * sequence_length, vocab_size]\n",
    "            labels = labels.view(-1)  # Shape [batch_size * sequence_length]\n",
    "            batch_teacher_logits = batch_teacher_logits.view(-1, student_logits.size(-1))  # Shape [batch_size * sequence_length, vocab_size]\n",
    "\n",
    "            # Compute the KD loss\n",
    "            loss = kd_loss(student_logits, batch_teacher_logits, labels)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Step the scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update the progress bar with the current loss\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(model, teacher_logits_L, data, tokenizer, optimizer, scheduler, kd_loss, num_epochs, device, batch_size)\n",
    "val = evaluate_model(model, val_data, tokenizer, device)\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save an initial copy of the model's state_dict\n",
    "initial_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "def objective(trial):\n",
    "    # Reset the model to its initial state\n",
    "    model.load_state_dict(initial_model_state)\n",
    "\n",
    "    # Suggest values for alpha and temperature\n",
    "    alpha = trial.suggest_float(\"alpha\", 0.0, 1.0)\n",
    "    temperature = 5.942267335064758\n",
    "\n",
    "    # Create the Knowledge Distillation loss function with the suggested parameters\n",
    "    kd_loss_fn = KnowledgeDistillationLoss(temperature=temperature, alpha=alpha).to(device)\n",
    "\n",
    "    # Reinitialize the optimizer and scheduler\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=5e-6, momentum=0.9, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(teacher_logits_L))\n",
    "\n",
    "    # Train the model using the `train_model` function\n",
    "    trained_model = train_model(\n",
    "        model=model,\n",
    "        teacher_logits_L=teacher_logits_L,\n",
    "        data=data,\n",
    "        tokenizer=tokenizer,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        kd_loss=kd_loss_fn,\n",
    "        num_epochs=num_epochs,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    val_loss = evaluate_model(trained_model, val_data, tokenizer, device)\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "# Define the number of trials\n",
    "n_trials = 35\n",
    "\n",
    "# Create the Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "# Add a progress bar for the study\n",
    "with tqdm(total=n_trials, desc=\"Temp/Alpha Trials\") as pbar:\n",
    "    def update_pbar(study, trial):\n",
    "        pbar.update(1)\n",
    "    \n",
    "    study.optimize(objective, n_trials=n_trials, callbacks=[update_pbar])\n",
    "\n",
    "with open(\"output2.txt\", \"w\") as file:\n",
    "    file.write(f\"Best alpha with temperature of {5.942267335064758}: {study.best_params['alpha']}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"For 1 ep, Best alpha: {study.best_params['alpha']}, Best temperature: {study.best_params['temperature']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"What is proof by induction?\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=3000, do_sample=True, top_p=0.9).to(device)\n",
    "output_answer = tokenizer.batch_decode(outputs)\n",
    "output_answer\n",
    "# with open(\"quadratic.txt\", \"w\") as file:\n",
    "#     file.write(output_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('llama8b-LoRA-IS', max_shard_size=\"5GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
