{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1bd9fad-aa5f-42ee-b953-ec06d34be5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, PretrainedConfig, LlamaForCausalLM\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from peft import get_peft_model, LoraConfig, PeftModel\n",
    "from random import sample\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from safetensors.torch import load_file\n",
    "import json\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "adapter_path = \"../../base-KD/llama8b-LoRA-IS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dfee39b-621f-471d-975d-28bb1b395cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e496570c36304557b9ca2cb73763c562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "student_tokenizer = AutoTokenizer.from_pretrained(\"../../Meta-Llama-3.1-8B-Instruct\")\n",
    "# student_base_model = AutoModelForCausalLM.from_pretrained(\"./Meta-Llama-3.1-8B\", torch_dtype=torch.bfloat16).to(device)\n",
    "student_trained_model = LlamaForCausalLM.from_pretrained(\"../../Meta-Llama-3.1-8B-Instruct\", torch_dtype=torch.bfloat16, device_map=device)\n",
    "student_trained_model = PeftModel.from_pretrained(student_trained_model, adapter_path) # testing trained\n",
    "student_trained_model = student_trained_model.merge_and_unload()\n",
    "\n",
    "# student_trained_model = get_peft_model(student_trained_model, lora_config)\n",
    "# student_trained_model.load_state_dict(load_file(\"../base-KD/llama8b-LoRA/model.safetensors\"), strict=True)\n",
    "# student_trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a796fb77-2e1e-40c4-bd05-a3de66dd84fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "# student_base_model.generation_config.pad_token_id = student_tokenizer.pad_token_id\n",
    "student_trained_model.generation_config.pad_token_id = student_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3031dd52-62f6-4dad-b39c-136cbd0dd586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Darrell and Allen's ages are in the ratio of 7:11. If their total age now is 162, calculate Allen's age 10 years from now. Allen's age after 10 years is the sum of his current age and the 10 years, if his current age is 11x. Therefore, Allen's age after 10 years is 11x + 10. Step 1: Set up the ratio between Darrell and Allen's ages\\nThe ratio between Darrell and Allen's ages is 7:11.\\nStep 2: Determine the total number of parts in the ratio\\n7 + 11 = 18\\nStep 3: Determine the value of 1 part\\n1 part = 162 / 18 = 9\\nStep 4: Determine Darrell's current age (7 parts)\\nDarrell's current age = 7 x 9 = 63\\nStep 5: Determine Allen's current age (11 parts)\\nAllen's current age = 11 x 9 = 99\\nStep 6: Calculate Allen's age 10 years from now\\nAllen's age after 10 years = 99 + 10 = 109\\nAllen's age 10 years from now is the sum of his current age and 10 years. We now know that Allen's current age is 11x, where x = 9. Therefore, Allen's age after 10 years is 11x + 10, which we know is 109.\\nStep 7: Solve for the value of Allen's current age\\n11x + 10 = 109\\nStep 8: Calculate the value of x\\nStep 9: Substitute the value of x in the equation 11x to determine Allen's current age\\n11(9) = 99\\nStep 10: Calculate Allen's age after 10 years\\nAllen's age after 10 years = 99 + 10 = 109\\nStep 11: Verify that Allen's age 10 years from now is indeed 109\\nAllen's age 10 years from now = 109\\nWe are asked\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs = student_tokenizer(\"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\", return_tensors=\"pt\").to(device)\n",
    "inputs = student_tokenizer(\"Darrell and Allen's ages are in the ratio of 7:11. If their total age now is 162, calculate Allen's age 10 years from now.\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "outputs = student_trained_model.generate(**inputs, max_new_tokens=400, do_sample=True, top_p=0.9, temperature=0.6).to(device)\n",
    "output_answer = student_tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "output_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e3d995-db3b-4125-ae9b-3e23f9372a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
    "data_samples = data.shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e1cc1ae-3913-48ce-b078-f3f65f4c50ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_answer(text):\n",
    "    \"\"\"\n",
    "    Extract the final numerical answer from a text using regex patterns.\n",
    "    \n",
    "    Args:\n",
    "        text: The text from which to extract the numerical answer.\n",
    "    \n",
    "    Returns:\n",
    "        final_answer: The extracted numerical answer as a float, or None if extraction fails.\n",
    "    \"\"\"\n",
    "    # 1. Try to find 'boxed' answers (e.g., \"The answer is \\boxed{42}\")\n",
    "    sepstr_extr = re.findall(r'\\\\boxed{ *([,\\d]+\\.?[,\\d]*) *}', text)\n",
    "    \n",
    "    # 2. Try to find matches in the first line\n",
    "    first_line = text.split('\\n')[0]\n",
    "    mtches = re.findall(r'[-+]?[$]?([,\\d]+\\.?[,\\d]*)\\%?(\\ |$|\\n|.){1}', first_line)\n",
    "    \n",
    "    # 3. Try to find phrases like 'the final answer is' and extract numbers from there\n",
    "    osepstr = re.findall(r'(the ?(final|correct)* ?answer ?is)', text, re.IGNORECASE)\n",
    "    oind = text.lower().find(osepstr[0][0].lower()) if osepstr else -1\n",
    "    oextr = re.findall(r'[-+]?[$]?([,\\d]+\\.?[,\\d]*)\\%?(\\ |$|\\n|.){1}', text[oind:]) if oind != -1 else []\n",
    "    \n",
    "    # 4. Determine the final answer\n",
    "    v = None\n",
    "    if sepstr_extr:\n",
    "        v = re.sub(r',', '', sepstr_extr[0])\n",
    "    elif len(mtches) == 1 or len(set([m[0] for m in mtches])) == 1:\n",
    "        v = re.sub(r',', '', mtches[0][0])\n",
    "    elif oextr:\n",
    "        v = re.sub(r',', '', oextr[0][0])\n",
    "    else:\n",
    "        extr = re.findall(r'[-+]?[$]?([,\\d]+\\.?[,\\d]*)\\%?(\\ |$|\\n|.){1}', text)\n",
    "        if len(extr) == 0:\n",
    "            # Could not extract any numbers\n",
    "            return None\n",
    "        v = re.sub(r',', '', extr[-1][0])\n",
    "    \n",
    "    try:\n",
    "        final_answer = float(v)\n",
    "    except:\n",
    "        final_answer = None\n",
    "    \n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5848cdc0-93d6-4a11-84e0-3828b7d98b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_logit_confidence(model, data_samples):\n",
    "    \"\"\"\n",
    "    Analyze the model's confidence in its outputs.\n",
    "\n",
    "    Args:\n",
    "        model: The language model.\n",
    "        data_samples: A list of dictionaries containing 'question' and 'answer'.\n",
    "\n",
    "    Returns:\n",
    "        confidences: A list of confidence scores.\n",
    "        labels: A list of correctness labels (1 for correct, 0 for incorrect).\n",
    "    \"\"\"\n",
    "    confidences = []\n",
    "    labels = []\n",
    "    for sample in tqdm(data_samples):\n",
    "        prompt = sample['question']\n",
    "        correct_answer_text = sample['answer']\n",
    "        # print(prompt)\n",
    "        \n",
    "        inputs = student_tokenizer(prompt, return_tensors='pt').to(device)\n",
    "        # Generate outputs without output_scores or return_dict_in_generate\n",
    "        outputs = student_trained_model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=350, \n",
    "            do_sample=True, \n",
    "            top_p=0.9, \n",
    "            temperature=0.6\n",
    "        ).to(device)\n",
    "        \n",
    "        # Get generated tokens\n",
    "        generated_sequence = outputs[0]  # shape: (sequence_length,)\n",
    "        generated_tokens = generated_sequence[inputs['input_ids'].shape[-1]:]\n",
    "        # print(len(generated_tokens))\n",
    "        \n",
    "        # Concatenate input_ids and generated tokens\n",
    "        full_sequence = generated_sequence  # Already contains inputs and generated tokens\n",
    "        \n",
    "        # Run the model on the full sequence to get logits\n",
    "        with torch.no_grad():\n",
    "            outputs_logits = student_trained_model(full_sequence.unsqueeze(0))\n",
    "            logits = outputs_logits.logits  # shape: (1, sequence_length, vocab_size)\n",
    "        \n",
    "        # Calculate confidence as average max logit at each step\n",
    "        input_length = inputs['input_ids'].shape[-1]\n",
    "        sequence_length = full_sequence.shape[0]\n",
    "        \n",
    "        start_idx = input_length - 1  # index of the last input token\n",
    "        end_idx = sequence_length - 1  # index of the last token in the sequence\n",
    "        \n",
    "        logits_to_use = logits[0, start_idx:end_idx, :]  # Exclude the last token's logits\n",
    "        \n",
    "        token_confidences = [torch.max(logit).item() for logit in logits_to_use]\n",
    "        avg_confidence = np.mean(token_confidences)\n",
    "        confidences.append(avg_confidence)\n",
    "        \n",
    "        # print(\"START\")\n",
    "        \n",
    "        # Decode generated answer\n",
    "        generated_answer = output_answer = student_tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "        \n",
    "        pattern = r'#{4}\\s+([+-]?\\d*\\.?\\d+)'\n",
    "        \n",
    "        match = re.search(pattern, correct_answer_text)\n",
    "        \n",
    "        if match:\n",
    "            correct_final_answer = float(match.group(1))\n",
    "            # print(f\"correct answer: {correct_final_answer}\")\n",
    "        else:\n",
    "            correct_final_answer = extract_final_answer(correct_answer_text)\n",
    "        \n",
    "        # print(correct_answer_text)\n",
    "        \n",
    "        # Extract numerical answers\n",
    "        model_final_answer = extract_final_answer(generated_answer)\n",
    "        # print()\n",
    "        # print(generated_answer)\n",
    "        # print()\n",
    "        # print(f\"model answer: {model_final_answer}\")\n",
    "        \n",
    "        # Check correctness\n",
    "        if model_final_answer is not None and correct_final_answer is not None:\n",
    "            is_correct = int(np.isclose(model_final_answer, correct_final_answer, atol=1e-3))\n",
    "        else:\n",
    "            # Fallback to string comparison\n",
    "            is_correct = int(generated_answer.strip().lower() == correct_answer_text.strip().lower())\n",
    "        labels.append(is_correct)\n",
    "        \n",
    "        # print(\"END\")\n",
    "    \n",
    "    return confidences, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff22271-e041-4bcd-8e36-b13d85e77642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      " 86%|████████▌ | 86/100 [14:37<02:24, 10.29s/it]"
     ]
    }
   ],
   "source": [
    "confidences, labels = analyze_logit_confidence(student_trained_model, data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a952fc-b8d6-4dc8-b737-b0f5712f6908",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Confidence': confidences, 'Correct': labels})\n",
    "sns.boxplot(x='Correct', y='Confidence', data=df)\n",
    "plt.title('Model Confidence for Correct vs Incorrect Answers')\n",
    "plt.xlabel('Answer Correctness (0=Incorrect, 1=Correct)')\n",
    "plt.ylabel('Average Max Logit Confidence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83495040-26bd-4b33-8eba-285666c5919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('model_confidence_data_basedistill.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
