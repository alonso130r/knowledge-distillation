{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1bd9fad-aa5f-42ee-b953-ec06d34be5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, PretrainedConfig, LlamaForCausalLM\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from peft import get_peft_model, LoraConfig, PeftModel\n",
    "from random import sample\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from safetensors.torch import load_file\n",
    "import json\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "adapter_path = \"../../base-2/llama8b-LoRA-IS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dfee39b-621f-471d-975d-28bb1b395cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cdf1adf110845f9918e3dd847f82a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "student_tokenizer = AutoTokenizer.from_pretrained(\"../../Meta-Llama-3.1-8B-Instruct\")\n",
    "# student_base_model = AutoModelForCausalLM.from_pretrained(\"./Meta-Llama-3.1-8B\", torch_dtype=torch.bfloat16).to(device)\n",
    "student_trained_model = LlamaForCausalLM.from_pretrained(\"../../Meta-Llama-3.1-8B-Instruct\", torch_dtype=torch.bfloat16, device_map=device)\n",
    "student_trained_model = PeftModel.from_pretrained(student_trained_model, adapter_path) # testing trained\n",
    "student_trained_model = student_trained_model.merge_and_unload()\n",
    "\n",
    "# student_trained_model = get_peft_model(student_trained_model, lora_config)\n",
    "# student_trained_model.load_state_dict(load_file(\"../base-KD/llama8b-LoRA/model.safetensors\"), strict=True)\n",
    "# student_trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a796fb77-2e1e-40c4-bd05-a3de66dd84fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "# student_base_model.generation_config.pad_token_id = student_tokenizer.pad_token_id\n",
    "student_trained_model.generation_config.pad_token_id = student_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3031dd52-62f6-4dad-b39c-136cbd0dd586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Darrell and Allen's ages are in the ratio of 7:11. If their total age now is 162, calculate Allen's age 10 years from now. [Answer: 53] I have done the solution for this problem in a different way. The following solution is the one you can follow and understand in a simple way.\\n## Step 1: Represent the ages of Darrell and Allen as 7x and 11x respectively\\nLet's assume Darrell's age is 7x and Allen's age is 11x, where x is a variable. This means the ratio of their ages is given as 7:11.\\n\\n## Step 2: Create an equation using the ages and the total age.\\nWe know that Darrell's age + Allen's age = Total age. In our case, it's 7x + 11x = 162. The total age of Darrell and Allen is 162.\\n\\n## Step 3: Combine the like terms and solve the equation.\\nNow, let's solve the equation. Combining the terms, we get 18x = 162.\\n\\n## Step 4: Solve the variable in the equation.\\nTo solve the variable x, let's divide both sides of the equation by 18. x = 162 / 18. x = 9.\\n\\n## Step 5: Determine the current age of Allen.\\nUsing the value of x as 9, let's find Allen's age using 11x. Allen's age is 11 * 9 = 99.\\n\\n## Step 6: Calculate Allen's age 10 years later.\\nAllen's age 10 years later = 99 + 10 = 108, which does not match the answer. There is something wrong in this solution. I have done this solution. But the real answer was 53.\\n\\n## Step 7: Calculate Allen's age using the correct method.\\nLet's consider the current age of Allen = 7/11 * 162 =  108. Allen's age 10 years later is = 108 + 10 = 118.  Now, the\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs = student_tokenizer(\"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\", return_tensors=\"pt\").to(device)\n",
    "inputs = student_tokenizer(\"Darrell and Allen's ages are in the ratio of 7:11. If their total age now is 162, calculate Allen's age 10 years from now.\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "outputs = student_trained_model.generate(**inputs, max_new_tokens=400, do_sample=True, top_p=0.9, temperature=0.6).to(device)\n",
    "output_answer = student_tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "output_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e3d995-db3b-4125-ae9b-3e23f9372a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
    "data_samples = data.shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e1cc1ae-3913-48ce-b078-f3f65f4c50ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_answer(text):\n",
    "    \"\"\"\n",
    "    Extract the final numerical answer from a text using regex patterns.\n",
    "    \n",
    "    Args:\n",
    "        text: The text from which to extract the numerical answer.\n",
    "    \n",
    "    Returns:\n",
    "        final_answer: The extracted numerical answer as a float, or None if extraction fails.\n",
    "    \"\"\"\n",
    "    # 1. Try to find 'boxed' answers (e.g., \"The answer is \\boxed{42}\")\n",
    "    sepstr_extr = re.findall(r'\\\\boxed{ *([,\\d]+\\.?[,\\d]*) *}', text)\n",
    "    \n",
    "    # 2. Try to find matches in the first line\n",
    "    first_line = text.split('\\n')[0]\n",
    "    mtches = re.findall(r'[-+]?[$]?([,\\d]+\\.?[,\\d]*)\\%?(\\ |$|\\n|.){1}', first_line)\n",
    "    \n",
    "    # 3. Try to find phrases like 'the final answer is' and extract numbers from there\n",
    "    osepstr = re.findall(r'(the ?(final|correct)* ?answer ?is)', text, re.IGNORECASE)\n",
    "    oind = text.lower().find(osepstr[0][0].lower()) if osepstr else -1\n",
    "    oextr = re.findall(r'[-+]?[$]?([,\\d]+\\.?[,\\d]*)\\%?(\\ |$|\\n|.){1}', text[oind:]) if oind != -1 else []\n",
    "    \n",
    "    # 4. Determine the final answer\n",
    "    v = None\n",
    "    if sepstr_extr:\n",
    "        v = re.sub(r',', '', sepstr_extr[0])\n",
    "    elif len(mtches) == 1 or len(set([m[0] for m in mtches])) == 1:\n",
    "        v = re.sub(r',', '', mtches[0][0])\n",
    "    elif oextr:\n",
    "        v = re.sub(r',', '', oextr[0][0])\n",
    "    else:\n",
    "        extr = re.findall(r'[-+]?[$]?([,\\d]+\\.?[,\\d]*)\\%?(\\ |$|\\n|.){1}', text)\n",
    "        if len(extr) == 0:\n",
    "            # Could not extract any numbers\n",
    "            return None\n",
    "        v = re.sub(r',', '', extr[-1][0])\n",
    "    \n",
    "    try:\n",
    "        final_answer = float(v)\n",
    "    except:\n",
    "        final_answer = None\n",
    "    \n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5848cdc0-93d6-4a11-84e0-3828b7d98b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_logit_confidence(model, data_samples):\n",
    "    \"\"\"\n",
    "    Analyze the model's confidence in its outputs.\n",
    "\n",
    "    Args:\n",
    "        model: The language model.\n",
    "        data_samples: A list of dictionaries containing 'question' and 'answer'.\n",
    "\n",
    "    Returns:\n",
    "        confidences: A list of confidence scores.\n",
    "        labels: A list of correctness labels (1 for correct, 0 for incorrect).\n",
    "    \"\"\"\n",
    "    confidences = []\n",
    "    labels = []\n",
    "    for sample in tqdm(data_samples):\n",
    "        prompt = sample['question']\n",
    "        correct_answer_text = sample['answer']\n",
    "        # print(prompt)\n",
    "        \n",
    "        inputs = student_tokenizer(prompt, return_tensors='pt').to(device)\n",
    "        # Generate outputs without output_scores or return_dict_in_generate\n",
    "        outputs = student_trained_model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=350, \n",
    "            do_sample=True, \n",
    "            top_p=0.9, \n",
    "            temperature=0.6\n",
    "        ).to(device)\n",
    "        \n",
    "        # Get generated tokens\n",
    "        generated_sequence = outputs[0]  # shape: (sequence_length,)\n",
    "        generated_tokens = generated_sequence[inputs['input_ids'].shape[-1]:]\n",
    "        # print(len(generated_tokens))\n",
    "        \n",
    "        # Concatenate input_ids and generated tokens\n",
    "        full_sequence = generated_sequence  # Already contains inputs and generated tokens\n",
    "        \n",
    "        # Run the model on the full sequence to get logits\n",
    "        with torch.no_grad():\n",
    "            outputs_logits = student_trained_model(full_sequence.unsqueeze(0))\n",
    "            logits = outputs_logits.logits  # shape: (1, sequence_length, vocab_size)\n",
    "        \n",
    "        # Calculate confidence as average max logit at each step\n",
    "        input_length = inputs['input_ids'].shape[-1]\n",
    "        sequence_length = full_sequence.shape[0]\n",
    "        \n",
    "        start_idx = input_length - 1  # index of the last input token\n",
    "        end_idx = sequence_length - 1  # index of the last token in the sequence\n",
    "        \n",
    "        logits_to_use = logits[0, start_idx:end_idx, :]  # Exclude the last token's logits\n",
    "        \n",
    "        token_confidences = [torch.max(logit).item() for logit in logits_to_use]\n",
    "        avg_confidence = np.mean(token_confidences)\n",
    "        confidences.append(avg_confidence)\n",
    "        \n",
    "        # print(\"START\")\n",
    "        \n",
    "        # Decode generated answer\n",
    "        generated_answer = output_answer = student_tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "        \n",
    "        pattern = r'#{4}\\s+([+-]?\\d*\\.?\\d+)'\n",
    "        \n",
    "        match = re.search(pattern, correct_answer_text)\n",
    "        \n",
    "        if match:\n",
    "            correct_final_answer = float(match.group(1))\n",
    "            # print(f\"correct answer: {correct_final_answer}\")\n",
    "        else:\n",
    "            correct_final_answer = extract_final_answer(correct_answer_text)\n",
    "        \n",
    "        # print(correct_answer_text)\n",
    "        \n",
    "        # Extract numerical answers\n",
    "        model_final_answer = extract_final_answer(generated_answer)\n",
    "        # print()\n",
    "        # print(generated_answer)\n",
    "        # print()\n",
    "        # print(f\"model answer: {model_final_answer}\")\n",
    "        \n",
    "        # Check correctness\n",
    "        if model_final_answer is not None and correct_final_answer is not None:\n",
    "            is_correct = int(np.isclose(model_final_answer, correct_final_answer, atol=1e-3))\n",
    "        else:\n",
    "            # Fallback to string comparison\n",
    "            is_correct = int(generated_answer.strip().lower() == correct_answer_text.strip().lower())\n",
    "        labels.append(is_correct)\n",
    "        \n",
    "        # print(\"END\")\n",
    "    \n",
    "    return confidences, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff22271-e041-4bcd-8e36-b13d85e77642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      " 46%|████▌     | 46/100 [08:02<09:26, 10.48s/it]"
     ]
    }
   ],
   "source": [
    "confidences, labels = analyze_logit_confidence(student_trained_model, data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a952fc-b8d6-4dc8-b737-b0f5712f6908",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Confidence': confidences, 'Correct': labels})\n",
    "sns.boxplot(x='Correct', y='Confidence', data=df)\n",
    "plt.title('Model Confidence for Correct vs Incorrect Answers')\n",
    "plt.xlabel('Answer Correctness (0=Incorrect, 1=Correct)')\n",
    "plt.ylabel('Average Max Logit Confidence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83495040-26bd-4b33-8eba-285666c5919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('model_confidence_data_base2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
