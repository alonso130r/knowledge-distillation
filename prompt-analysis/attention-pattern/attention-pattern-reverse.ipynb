{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87bb6cfc-3c1e-448e-9038-711e84a610cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from peft import get_peft_model, LoraConfig, PeftModel\n",
    "from random import sample\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "adapter_path = \"../../reverseKL-KD/llama8b-LoRA-IS\"\n",
    "save_dir = \"heatmap-reverse/\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d75e9261-f222-4cfc-afde-716e98c64745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd56d1df898f4889a61ef7b3762081c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_tokenizer = AutoTokenizer.from_pretrained(\"../../Meta-Llama-3.1-8B\")\n",
    "student_trained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"../../Meta-Llama-3.1-8B\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device\n",
    ")\n",
    "student_trained_model = PeftModel.from_pretrained(student_trained_model, adapter_path) # testing trained\n",
    "student_trained_model = student_trained_model.merge_and_unload()\n",
    "student_trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b1d29c9-5f5d-46ba-8b27-36cbd1eb5e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_trained_model.config.output_attentions = True\n",
    "student_tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "student_trained_model.generation_config.pad_token_id = student_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fcdd3d6-a2e7-4ceb-963c-c24c8db4ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_llama_attention_all_heads(model, tokenizer, prompt, layer_num, save_dir):\n",
    "    \"\"\"\n",
    "    Visualize and save attention weights for all heads in a specific layer of a LLaMA model.\n",
    "\n",
    "    Args:\n",
    "        model: The LLaMA language model.\n",
    "        tokenizer: The tokenizer associated with the model.\n",
    "        prompt: Input prompt string.\n",
    "        layer_num: The layer number to visualize (0-indexed).\n",
    "        save_dir: Directory where the images will be saved.\n",
    "\n",
    "    Returns:\n",
    "        Saves heatmaps of attention weights for all heads in the specified layer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Prepare the input\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    input_ids = inputs['input_ids']\n",
    "\n",
    "    # Ensure the model outputs attentions\n",
    "    model.config.output_attentions = True\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Extract attention weights from the outputs\n",
    "    attentions = outputs.attentions  # List of attention tensors for each layer\n",
    "\n",
    "    # Get attention weights for the specified layer\n",
    "    attn = attentions[layer_num][0]  # Shape: (num_heads, seq_len, seq_len)\n",
    "\n",
    "    num_heads = attn.shape[0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate over all heads and plot attention weights\n",
    "    for head_num in range(num_heads):\n",
    "        head_attn = attn[head_num]  # Shape: (seq_len, seq_len)\n",
    "        # Cast to float32 to avoid TypeError\n",
    "        head_attn_float = head_attn.cpu().to(dtype=torch.float32)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(\n",
    "            head_attn_float.numpy(),\n",
    "            xticklabels=tokens,\n",
    "            yticklabels=tokens,\n",
    "            cmap='viridis'\n",
    "        )\n",
    "        plt.title(f'Attention Weights - Layer {layer_num}, Head {head_num}')\n",
    "        plt.xlabel('Key Tokens')\n",
    "        plt.ylabel('Query Tokens')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Construct the filename\n",
    "        filename = f'layer_{layer_num}_head_{head_num}.png'\n",
    "        filepath = os.path.join(save_dir, filename)\n",
    "\n",
    "        # Save the figure\n",
    "        plt.savefig(filepath)\n",
    "        plt.close()  # Close the figure to free up memory\n",
    "\n",
    "        # print(f'Saved heatmap to {filepath}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e35555ae-9db3-4799-a4d9-65767d615360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\"\n",
    "\n",
    "for layer in range(31):\n",
    "    # Visualize attention for all heads in the specified layer\n",
    "    visualize_llama_attention_all_heads(student_trained_model, student_tokenizer, prompt, layer, save_dir + f\"layer{layer}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
