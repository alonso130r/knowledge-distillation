{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from safetensors.torch import save_file\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import List\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-405B-FP8\", token=os.getenv(\"HUGGINGFACE\"))\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-405B-FP8\", token=os.getenv(\"HUGGINGFACE\"), device_map=\"cuda\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
    "questions = ds[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(inputs : str) -> torch.tensor:\n",
    "    inputs = tokenizer(inputs, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        return outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_logits(logits : List[torch.tensor], ) -> None:\n",
    "    data = {}\n",
    "    for i, logit in enumerate(logits):\n",
    "        data[f\"Question {i+1}\"] = logit.cpu()\n",
    "\n",
    "    save_file(data, \"llama-3.1-405b-gsm8k-base-tensors.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be used later on to load back the logits\n",
    "\n",
    "from safetensors.torch import safe_open\n",
    "\n",
    "def load_list_of_logits_safetensor(file_path):\n",
    "    # Open the safetensor file\n",
    "    with safe_open(file_path, framework=\"pt\") as f:\n",
    "        logits_list = []\n",
    "        for key in f.keys():\n",
    "            logits_list.append(f.get_tensor(key))\n",
    "    \n",
    "    return logits_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = [get_logits(question) for question in questions]\n",
    "save_logits(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RuntimeError: Failed to import transformers.integrations.fbgemm_fp8 because of the following error (look up to see its traceback):\n",
    "libcudart.so.12: cannot open shared object file: No such file or directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linux-vdso.so.1 (0x00007ffe239fe000)\n",
    "        libtorch.so => not found (wtf??????)\n",
    "        libc10.so => not found   \n",
    "        libcudart.so.12 => /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so.12 (0x000073e204e00000) (why did this say not found???)\n",
    "        libc10_cuda.so => not found (issue)\n",
    "        libnvidia-ml.so.1 => /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1 (0x000073e203c00000)\n",
    "        libtorch_cpu.so => not found\n",
    "        libtorch_cuda.so => not found (massive issue)\n",
    "        librt.so.1 => /usr/lib/x86_64-linux-gnu/librt.so.1 (0x000073e20513a000)\n",
    "        libpthread.so.0 => /usr/lib/x86_64-linux-gnu/libpthread.so.0 (0x000073e205135000)\n",
    "        libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x000073e2039d4000)\n",
    "        libm.so.6 => /usr/lib/x86_64-linux-gnu/libm.so.6 (0x000073e204d19000)\n",
    "        libgcc_s.so.1 => /usr/lib/x86_64-linux-gnu/libgcc_s.so.1 (0x000073e205113000)\n",
    "        libc.so.6 => /usr/lib/x86_64-linux-gnu/libc.so.6 (0x000073e2037ab000)\n",
    "        /lib64/ld-linux-x86-64.so.2 (0x000073e25e59b000)\n",
    "        libdl.so.2 => /usr/lib/x86_64-linux-gnu/libdl.so.2 (0x000073e20510e000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "Cell In[3], line 2\n",
    "      1 tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-405B-FP8\", token=os.getenv(\"HUGGINGFACE\"))\n",
    "----> 2 model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-405B-FP8\", token=os.getenv(\"HUGGINGFACE\"), device_map='cuda').to(device)\n",
    "      3 model.eval()\n",
    "\n",
    "File /usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:564, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\n",
    "    562 elif type(config) in cls._model_mapping.keys():\n",
    "    563     model_class = _get_model_class(config, cls._model_mapping)\n",
    "--> 564     return model_class.from_pretrained(\n",
    "    565         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n",
    "    566     )\n",
    "    567 raise ValueError(\n",
    "    568     f\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\n",
    "    569     f\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\n",
    "    570 )\n",
    "\n",
    "File /usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3941, in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\n",
    "   3931     if dtype_orig is not None:\n",
    "   3932         torch.set_default_dtype(dtype_orig)\n",
    "   3934     (\n",
    "   3935         model,\n",
    "   3936         missing_keys,\n",
    "   3937         unexpected_keys,\n",
    "   3938         mismatched_keys,\n",
    "   3939         offload_index,\n",
    "   3940         error_msgs,\n",
    "-> 3941     ) = cls._load_pretrained_model(\n",
    "   3942         model,\n",
    "   3943         state_dict,\n",
    "   3944         loaded_state_dict_keys,  # XXX: rename?\n",
    "   3945         resolved_archive_file,\n",
    "   3946         pretrained_model_name_or_path,\n",
    "   3947         ignore_mismatched_sizes=ignore_mismatched_sizes,\n",
    "   3948         sharded_metadata=sharded_metadata,\n",
    "   3949         _fast_init=_fast_init,\n",
    "   3950         low_cpu_mem_usage=low_cpu_mem_usage,\n",
    "   3951         device_map=device_map,\n",
    "   3952         offload_folder=offload_folder,\n",
    "   3953         offload_state_dict=offload_state_dict,\n",
    "   3954         dtype=torch_dtype,\n",
    "   3955         hf_quantizer=hf_quantizer,\n",
    "   3956         keep_in_fp32_modules=keep_in_fp32_modules,\n",
    "   3957         gguf_path=gguf_path,\n",
    "   3958     )\n",
    "   3960 # make sure token embedding weights are still tied if needed\n",
    "   3961 model.tie_weights()\n",
    "\n",
    "File /usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4415, in PreTrainedModel._load_pretrained_model(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\n",
    "   4411                 set_module_tensor_to_device(\n",
    "   4412                     model_to_load, key, \"cpu\", torch.empty(*param.size(), dtype=dtype)\n",
    "   4413                 )\n",
    "   4414     else:\n",
    "-> 4415         new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n",
    "   4416             model_to_load,\n",
    "   4417             state_dict,\n",
    "   4418             loaded_keys,\n",
    "   4419             start_prefix,\n",
    "   4420             expected_keys,\n",
    "   4421             device_map=device_map,\n",
    "   4422             offload_folder=offload_folder,\n",
    "   4423             offload_index=offload_index,\n",
    "   4424             state_dict_folder=state_dict_folder,\n",
    "   4425             state_dict_index=state_dict_index,\n",
    "   4426             dtype=dtype,\n",
    "   4427             hf_quantizer=hf_quantizer,\n",
    "   4428             is_safetensors=is_safetensors,\n",
    "   4429             keep_in_fp32_modules=keep_in_fp32_modules,\n",
    "   4430             unexpected_keys=unexpected_keys,\n",
    "   4431         )\n",
    "   4432         error_msgs += new_error_msgs\n",
    "   4433 else:\n",
    "   4434     # Sharded checkpoint or whole but low_cpu_mem_usage==True\n",
    "\n",
    "File /usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:936, in _load_state_dict_into_meta_model(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\n",
    "    925     state_dict_index = offload_weight(param, param_name, state_dict_folder, state_dict_index)\n",
    "    926 elif (\n",
    "    927     not is_quantized\n",
    "    928     or (not hf_quantizer.requires_parameters_quantization)\n",
    "   (...)\n",
    "    934 ):\n",
    "    935     # For backward compatibility with older versions of `accelerate` and for non-quantized params\n",
    "--> 936     set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)\n",
    "    937 else:\n",
    "    938     hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
    "\n",
    "File /usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:373, in set_module_tensor_to_device(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\n",
    "    369 if value is not None:\n",
    "    370     # We can expect mismatches when using bnb 4bit since Params4bit will reshape and pack the weights.\n",
    "    371     # In other cases, we want to make sure we're not loading checkpoints that do not match the config.\n",
    "    372     if old_value.shape != value.shape and param_cls.__name__ != \"Params4bit\":\n",
    "--> 373         raise ValueError(\n",
    "    374             f'Trying to set a tensor of shape {value.shape} in \"{tensor_name}\" (which has shape {old_value.shape}), this looks incorrect.'\n",
    "    375         )\n",
    "    377     if dtype is None:\n",
    "    378         # For compatibility with PyTorch load_state_dict which converts state dict dtype to existing dtype in model\n",
    "    379         value = value.to(old_value.dtype)\n",
    "\n",
    "ValueError: Trying to set a tensor of shape torch.Size([1024, 16384]) in \"weight\" (which has shape torch.Size([2048, 16384])), this looks incorrect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
