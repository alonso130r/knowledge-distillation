{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# import torch\n",
    "import pandas as pd\n",
    "import openai\n",
    "import re\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import tiktoken\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    " #print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"derive the quadratic formula.\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To derive the quadratic formula, we start with the general form of a quadratic equation:\\n\\n\\\\[\\nax^2 + bx + c = 0\\n\\\\]\\n\\nwhere \\\\(a\\\\), \\\\(b\\\\), and \\\\(c\\\\) are constants, and \\\\(a \\\\neq 0\\\\). We want to solve for \\\\(x\\\\). The first step is to move the constant term to the other side of the equation:\\n\\n\\\\[\\nax^2 + bx = -c\\n\\\\]\\n\\nNext, we will complete the square for the left-hand side. To do this, we first divide the entire equation by \\\\(a\\\\) to make the coefficient of \\\\(x^2\\\\) equal to 1:\\n\\n\\\\[\\nx^2 + \\\\frac{b}{a}x = -\\\\frac{c}{a}\\n\\\\]\\n\\nNow, we need to complete the square. The term needed to complete the square involves taking half of the coefficient of \\\\(x\\\\) (which is \\\\(\\\\frac{b}{a}\\\\)), squaring it, and adding it to both sides. Half of \\\\(\\\\frac{b}{a}\\\\) is \\\\(\\\\frac{b}{2a}\\\\), and squaring this gives:\\n\\n\\\\[\\n\\\\left(\\\\frac{b}{2a}\\\\right)^2 = \\\\frac{b^2}{4a^2}\\n\\\\]\\n\\nWe add this term to both sides:\\n\\n\\\\[\\nx^2 + \\\\frac{b}{a}x + \\\\frac{b^2}{4a^2} = -\\\\frac{c}{a} + \\\\frac{b^2}{4a^2}\\n\\\\]\\n\\nThe left-hand side can now be factored as a perfect square:\\n\\n\\\\[\\n\\\\left(x + \\\\frac{b}{2a}\\\\right)^2 = -\\\\frac{c}{a} + \\\\frac{b^2}{4a^2}\\n\\\\]\\n\\nWe can combine the right-hand side. To do this, we express \\\\(-\\\\frac{c}{a}\\\\) with a common denominator of \\\\(4a^2\\\\):\\n\\n\\\\[\\n-\\\\frac{c}{a} = -\\\\frac{4ac}{4a^2} \\\\implies -\\\\frac{c}{a} + \\\\frac{b^2}{4a^2} = \\\\frac{b^2 - 4ac}{4a^2}\\n\\\\]\\n\\nNow, our equation looks like this:\\n\\n\\\\[\\n\\\\left(x + \\\\frac{b}{2a}\\\\right)^2 = \\\\frac{b^2 - 4ac}{4a^2}\\n\\\\]\\n\\nNext, we take the square root of both sides. Remember to consider both the positive and negative square roots:\\n\\n\\\\[\\nx + \\\\frac{b}{2a} = \\\\pm \\\\frac{\\\\sqrt{b^2 - 4ac}}{2a}\\n\\\\]\\n\\nNow, we solve for \\\\(x\\\\) by isolating it:\\n\\n\\\\[\\nx = -\\\\frac{b}{2a} \\\\pm \\\\frac{\\\\sqrt{b^2 - 4ac}}{2a}\\n\\\\]\\n\\nCombining the fractions, we have:\\n\\n\\\\[\\nx = \\\\frac{-b \\\\pm \\\\sqrt{b^2 - 4ac}}{2a}\\n\\\\]\\n\\nThis gives us the quadratic formula, which can be used to find the roots of any quadratic equation:\\n\\n\\\\[\\nx = \\\\frac{-b \\\\pm \\\\sqrt{b^2 - 4ac}}{2a}\\n\\\\]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "gsm8k_test = gsm8k['test'].to_pandas()\n",
    "gsm8k_train = gsm8k['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompts(df, model : str, dist_folder : str, subset : str, prompt_function, start, end):\n",
    "    df['model_prompt'] = ''\n",
    "    for i, row in df[start:end].iterrows():\n",
    "        df.loc[i, 'model_prompt'] = prompt_function(row['question'])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from async_client import batch_file\n",
    "def open_batch(df, batch_name : str, model : str, dist_folder : str, subset : str):\n",
    "    messages = []\n",
    "    for i, row in df.iterrows():\n",
    "        p = row['model_prompt']\n",
    "        msg = {\n",
    "                'messages' : [\n",
    "                    {\n",
    "                        'role' : 'system',\n",
    "                        'content' : ''\n",
    "                    },\n",
    "                    {\n",
    "                        'role' : 'user',\n",
    "                        'content' : p\n",
    "                    }\n",
    "                    ]\n",
    "                }\n",
    "        messages.append(msg)\n",
    "    \n",
    "    file_path = batch_file(batch_name, model, messages, client)\n",
    "    return file_path\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(df : pd.DataFrame, model : str, dist_folder : str, edition : str):\n",
    "    base_path = Path()\n",
    "    base_path = base_path / '..' / '..' / 'logs' / model / dist_folder / f'{edition}.csv'\n",
    "    df.to_csv(base_path)\n",
    "\n",
    "def assess_model(df, model : str, dist_folder : str, subset : str, prompt_function, model_respond, max_tokens = 50, start = 0, end = None):\n",
    "    numrows = df.shape[0]\n",
    "    if not end:\n",
    "        end = numrows\n",
    "    i = 0\n",
    "    df['model_answer'] = ''\n",
    "    df['model_output_tokens'] = 0\n",
    "    df['model_max_tokens'] = False\n",
    "    for i, row in df[start:end].iterrows():\n",
    "\n",
    "        # tokens = tokenizer(row['question'])\n",
    "        # val, out_tokens = generator(tokens, max_tokens)\n",
    "        \n",
    "        out_token_count = len(out_tokens[0].tolist()) - len(tokens['input_ids'][0].tolist())\n",
    "        df.loc[i, 'model_answer'] = val\n",
    "        df.loc[i, 'model_output_tokens'] = out_token_count\n",
    "        df.loc[i, 'model_max_tokens'] = True if out_token_count == max_tokens else False\n",
    "        i += 1\n",
    "        if i % 5 == 0:\n",
    "            print(f'Completed : {i} out of {numrows}')\n",
    "            print(f'Completed : {round(i/numrows, 4) * 100}%')\n",
    "            print('\\n')\n",
    "        if i % 10 == 0:\n",
    "            print(f'Completed : {i} out of {numrows}')\n",
    "            save_df(df, model, dist_folder, f'intermediate_{i}')\n",
    "    save_df(df, model, dist_folder, f'FINAL_Completed({start} : {end})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_prompt(question):\n",
    "    return question + '\\n Please respond with just the answer. The answer is:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_tokenizerd(prompt):\n",
    "    tokens = student_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_generation(tokens, max_tokens=50):\n",
    "    outputs = student_model.generate(**tokens, max_new_tokens=max_tokens, do_sample=False).to(device)\n",
    "    output_answer = student_tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    return output_answer, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi, to = student_generation(student_tokenizerd(default_prompt(gsm8k_test['question'][6])), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts(gsm8k_test, 'gpt-4o-mini', 'no-distil-test', 'test', default_prompt, 0, 1319)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df(gsm8k_test, 'gpt-4o-mini', 'no-distil-test', 'prompts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../logs/gpt-4o-mini/init_assessment-gpt4o-mini-distill.jsonl')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_batch(gsm8k_test, 'init_assessment-gpt4o-mini-distill', 'gpt-4o-mini', '', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assess_model(gsm8k_test, 'gemma2b', 'no-distil-test', 'test', student_tokenizerd, student_generation, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_disk(model : str, dist_folder : str, edition : str = 'FINAL'):\n",
    "    base_path = Path()\n",
    "    base_path = base_path / '..' / 'logs' / model / dist_folder / edition\n",
    "    return load_from_disk(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_data = load_disk('gpt2-xl', 'no-distil-test', 'intermediate_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 1319\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsm8k_data['test']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
