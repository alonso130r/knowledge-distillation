{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1679c86565ea439a886ec0c40c9a81b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe16332e738340c9a3436635e1e89a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954c889b302d41609571ada38a852a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871bd3b9bbcd43a4b3daf013a41eaa78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e080644ed00f427da0291b309b5dbfc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2683706001e34e3b92ddb6333271b43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427a1c8c31e44c1895d246ca34993d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb1771f5454948f6a01575bd3db586b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ad9a1696bc41d9a48904f96ed131a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c550402bff1242d889375df0a12971c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a883bfcfa6194a67acc9345f14cb886c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# student model\n",
    "\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\") # Google also released Gemma 2 - 2b, use the following id for the newer model: \"google/gemma-2-2b-it\"\n",
    "student_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    torch_dtype=torch.float32\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "student_model.generation_config.pad_token_id = student_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quadratic formula is a very useful tool for solving problems involving the quadratic equation. It is also used to solve problems involving the cubic equation.\\n\\nThe quadratic formula is a very useful tool for solving problems involving the quadratic equation. It is'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing cell\n",
    "inputs = student_tokenizer('The quadratic formula is', return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "\n",
    "outputs = student_model.generate(**inputs, max_new_tokens=50, do_sample=False, truncate=True, eos_token_id=50256, ).to(device)\n",
    "output_answer = student_tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "output_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = student_tokenizer('The quadratic formula is a very useful tool for solving problems involving the quadratic equation. It is also used to solve problems involving the cubic equation.\\n\\nThe quadratic formula is a very useful tool for solving problems involving the quadratic equation. It is also used to solve problems involving the cubic equation.\\n\\nThe quadratic formula is a very useful tool for solving problems involving the quadratic equation. It is also used to solve problems involving the cubic equation.\\n\\nThe quadratic formula is a very useful tool for solving problems involving the quadratic equation. It is also used to solve problems involving the cubic equation.\\n\\nThe quadratic formula is a very useful tool for solving problems involving the quadratic equation.', return_tensors=\"pt\")\n",
    "len(t['input_ids'][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "gsm8k_test = gsm8k['test'].to_pandas()\n",
    "gsm8k_train = gsm8k['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(df : pd.DataFrame, model : str, dist_folder : str, edition : str):\n",
    "    base_path = Path()\n",
    "    base_path = base_path / '..' / 'logs' / model / dist_folder / f'{edition}.csv'\n",
    "    df.to_csv(base_path)\n",
    "\n",
    "def assess_model(df, model : str, dist_folder : str, subset : str, prompt_function, tokenizer, generator, max_tokens = 50, start = 0, end = None):\n",
    "    numrows = df.shape[0]\n",
    "    if not end:\n",
    "        end = numrows\n",
    "    i = 0\n",
    "    df['model_prompt'] = ''\n",
    "    df['model_answer'] = ''\n",
    "    df['model_output_tokens'] = 0\n",
    "    df['model_max_tokens'] = False\n",
    "    for i, row in df[start:end].iterrows():\n",
    "        df.loc[i, 'model_prompt'] = \n",
    "        tokens = tokenizer(row['question'])\n",
    "        val, out_tokens = generator(tokens, max_tokens)\n",
    "        out_token_count = len(out_tokens[0].tolist()) - len(tokens['input_ids'][0].tolist())\n",
    "        df.loc[i, 'model_answer'] = val\n",
    "        df.loc[i, 'model_output_tokens'] = out_token_count\n",
    "        df.loc[i, 'model_max_tokens'] = True if out_token_count == max_tokens else False\n",
    "        i += 1\n",
    "        if i % 5 == 0:\n",
    "            print(f'Completed : {i} out of {numrows}')\n",
    "            print(f'Completed : {round(i/numrows, 4) * 100}%')\n",
    "            print('\\n')\n",
    "        if i % 10 == 0:\n",
    "            print(f'Completed : {i} out of {numrows}')\n",
    "            save_df(df, model, dist_folder, f'intermediate_{i}')\n",
    "    save_df(df, model, dist_folder, f'FINAL_Completed({start} : {end})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_prompt(question):\n",
    "    return question + '\\n Please respond with just the answer. The answer is:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_tokenizerd(prompt):\n",
    "    tokens = student_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_generation(tokens, max_tokens=50):\n",
    "    outputs = student_model.generate(**tokens, max_new_tokens=max_tokens, do_sample=False).to(device)\n",
    "    output_answer = student_tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    return output_answer, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi, to = student_generation(student_tokenizerd(default_prompt(gsm8k_test['question'][6])), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "transformers.generation.utils.GenerationMixin.generate() argument after ** must be a mapping, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43massess_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgsm8k_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgemma2-2b-it\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mno-distil-test\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstudent_tokenizerd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstudent_generation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m, in \u001b[0;36massess_model\u001b[0;34m(df, model, dist_folder, subset, prompt_function, tokenizer, generator, max_tokens, start, end)\u001b[0m\n\u001b[1;32m     13\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_max_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m df[start:end]\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m---> 15\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     val, out_tokens \u001b[38;5;241m=\u001b[39m generator(tokens, max_tokens)\n\u001b[1;32m     17\u001b[0m     out_token_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(out_tokens[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m, in \u001b[0;36mstudent_generation\u001b[0;34m(tokens, max_tokens)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstudent_generation\u001b[39m(tokens, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m student_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokens, max_new_tokens\u001b[38;5;241m=\u001b[39mmax_tokens, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m     output_answer \u001b[38;5;241m=\u001b[39m student_tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_answer, outputs\n",
      "\u001b[0;31mTypeError\u001b[0m: transformers.generation.utils.GenerationMixin.generate() argument after ** must be a mapping, not str"
     ]
    }
   ],
   "source": [
    "assess_model(gsm8k_test, 'gemma2b-it', 'no-distil-test', 'test', student_tokenizerd, student_generation, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_disk(model : str, dist_folder : str, edition : str = 'FINAL'):\n",
    "    base_path = Path()\n",
    "    base_path = base_path / '..' / 'logs' / model / dist_folder / edition\n",
    "    return load_from_disk(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_data = load_disk('gpt2-xl', 'no-distil-test', 'intermediate_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 1319\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsm8k_data['test']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
