{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)\n",
    "datapath = \"../data/24.csv\"\n",
    "instructions = \"\"\"\n",
    "Solve the following puzzle using the numbers provided to get the result 24. You are given four numbers, and you can use addition (+), subtraction (-), multiplication (*), and division (/) to combine these numbers. You must use each number exactly once. You can use parentheses to group operations and control the order of operations. Along with that, the numbers must stay in the order they are presented in. \n",
    "\n",
    "For example:\n",
    "Numbers: 1, 1, 8, 8\n",
    "Solution:\n",
    "1. Start with the numbers 1, 1, 8, and 8.\n",
    "2. Group 1 and 1 to get (1 + 1) = 2.\n",
    "3. Multiply 2 by 8 to get 2 * 8 = 16.\n",
    "4. Add the remaining 8 to get 16 + 8 = 24.\n",
    "Final answer: (1+1) * 8 + 8\n",
    "\n",
    "Another example:\n",
    "Numbers: 4, 2, 3, 1\n",
    "Solution:\n",
    "1. Start with the numbers 3, 3, 8, and 8.\n",
    "2. Group 4 and 1 to get (4 + 2) = 6.\n",
    "3. Group 3 and 1 to get (3 + 1) = 4. \n",
    "4. Multiply 6 by 4 to get 6 * 4 = 24\n",
    "Final answer: (4 + 2) * (3 + 1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student model\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-large\")\n",
    "student_model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 1280)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "student_model.resize_token_embeddings(len(student_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(datapath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "puzzles = df['Puzzles'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game24Dataset(Dataset):\n",
    "    def __init__(self, puzzles, tokenizer, instructions, max_length=512):\n",
    "        self.puzzles = puzzles\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.instructions = instructions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.puzzles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        puzzle = self.puzzles[idx]\n",
    "        prompt = f\"{self.instructions}: {puzzle}\"\n",
    "        encoding = self.tokenizer(\n",
    "            prompt,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': input_ids\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prototype of a loss function\n",
    "\n",
    "def enhanced_loss_function(outputs, puzzles, tokenizer):\n",
    "    total_loss = 0\n",
    "    batch_size = len(puzzles)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        solution = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "        puzzle = puzzles[i]\n",
    "        \n",
    "        # Check for correctness\n",
    "        correct, partial_loss = evaluate_solution(solution, puzzle)\n",
    "        \n",
    "        total_loss += partial_loss if not correct else 0\n",
    "    \n",
    "    return total_loss / batch_size\n",
    "\n",
    "def evaluate_solution(solution, puzzle):\n",
    "   # Extract numbers and operators from the solution\n",
    "    numbers = re.findall(r'\\d+', solution)\n",
    "    operators_and_brackets = re.findall(r'[+\\-*/()]', solution)\n",
    "    \n",
    "    # Ensure the numbers used are exactly the ones in the puzzle\n",
    "    puzzle_numbers = sorted(puzzle.split())\n",
    "    solution_numbers = sorted(numbers)\n",
    "    \n",
    "    # Calculate a partial loss based on the incorrect use of numbers\n",
    "    partial_loss = len(set(puzzle_numbers) - set(solution_numbers)) / len(puzzle_numbers)\n",
    "    \n",
    "    if puzzle_numbers != solution_numbers:\n",
    "        return False, 1 + partial_loss\n",
    "    \n",
    "    # Check if valid operators and brackets are used\n",
    "    valid_operators = set('+-*/')\n",
    "    valid_brackets = set('()')\n",
    "    invalid_chars = [char for char in operators_and_brackets if char not in valid_operators and char not in valid_brackets]\n",
    "    \n",
    "    if invalid_chars:\n",
    "        partial_loss += 0.5  # Arbitrary penalty for invalid operators or brackets\n",
    "    \n",
    "    # Check for balanced brackets\n",
    "    if not are_brackets_balanced(solution):\n",
    "        partial_loss += 0.5  # Arbitrary penalty for unbalanced brackets\n",
    "    \n",
    "    # Evaluate the expression\n",
    "    try:\n",
    "        result = eval(solution)\n",
    "        if result == 24:\n",
    "            return True, 0\n",
    "        else:\n",
    "            return False, abs(24 - result) / 24 + partial_loss\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating solution: {e}\")\n",
    "        return False, 1 + partial_loss \n",
    "\n",
    "def are_brackets_balanced(expression):\n",
    "    stack = []\n",
    "    brackets = {'(': ')'}\n",
    "    \n",
    "    for char in expression:\n",
    "        if char in brackets.keys():\n",
    "            stack.append(char)\n",
    "        elif char in brackets.values():\n",
    "            if not stack or brackets[stack.pop()] != char:\n",
    "                return False\n",
    "    return not stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Game24Dataset(puzzles, student_tokenizer, instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50258, 1280)\n",
       "    (wpe): Embedding(1024, 1280)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-35): 36 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1280, out_features=50258, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.eval()\n",
    "student_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_solution(generated_output, instructions):\n",
    "    try:\n",
    "        solution = generated_output.split(instructions)[-1].strip()\n",
    "        return solution\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting solution: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, tokenizer, progress_interval=100):\n",
    "    total_correct = 0\n",
    "    total_puzzles = 0\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Generate solutions from the model\n",
    "            generated_outputs = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=50)\n",
    "        \n",
    "        # Decode puzzles for evaluation\n",
    "        puzzles = [tokenizer.decode(ids, skip_special_tokens=True).replace(instructions, \"\").strip() for ids in input_ids]\n",
    "        \n",
    "        for j in range(len(generated_outputs)):\n",
    "            raw_solution = tokenizer.decode(generated_outputs[j], skip_special_tokens=True)\n",
    "            solution = extract_solution(raw_solution, instructions)\n",
    "            print(f\"solution {j}: {solution}\")\n",
    "            puzzle = puzzles[j]\n",
    "            correct, _ = evaluate_solution(solution, puzzle)\n",
    "            total_correct += int(correct)\n",
    "            total_puzzles += 1\n",
    "        \n",
    "        \n",
    "        accuracy = total_correct / total_puzzles if total_puzzles > 0 else 0\n",
    "        print(f\"Progress: {i + 1} batches processed. Current accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # Final accuracy\n",
    "    accuracy = total_correct / total_puzzles if total_puzzles > 0 else 0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "C:\\Users\\gites\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:650: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution 0: : 12 12 12 13:\n",
      ": 12 12 12 13:\n",
      ": 12 12 12 13:\n",
      ": 12 12 12 13:\n",
      ": 12 12 12 13:\n",
      ": 12 12 12 13:\n",
      ": 12 12 12 13:\n",
      ": 12 12 12 13:\n",
      "solution 1: : 4 4 7 13:\n",
      ": 4 4 7 13:\n",
      ": 4 4 7 13:\n",
      ": 4 4 7 13:\n",
      ": 4 4 7 13:\n",
      ": 4 4 7 13:\n",
      ": 4 4 7 13:\n",
      ": 4 4 7 13:\n",
      "solution 2: : 3 4 7 9:\n",
      ": 3 4 7 9:\n",
      ": 3 4 7 9:\n",
      ": 3 4 7 9:\n",
      ": 3 4 7 9:\n",
      ": 3 4 7 9:\n",
      ": 3 4 7 9:\n",
      ": 3 4 7 9:\n",
      "solution 3: : 2 2 6 6: 2 2 6 6: 2 2 6 6: 2 2 6 6: 2 2 6 6: 2 2 6 6: 2 2 6 6: 2 2 6 6: 2 2 6 6: 2 2 6 6: 2 2 6 6\n",
      "solution 4: : 1 3 5 11:\n",
      "\n",
      "1. Start with the numbers 1, 1, 8, 8.\n",
      "\n",
      "2. Group 1 and 1 to get (1 + 1) = 2.\n",
      "\n",
      "3. Multiply 2 by 8 to get 2 * 8\n",
      "solution 5: : 1 6 12 13: 2 6 12 13: 3 6 12 13: 4 6 12 13: 5 6 12 13: 6 6 12 13: 7 6 12 13: 8 6 12 13: 9 6 12 13: 10 6 12 13: 11 6 12 13\n",
      "solution 6: : 4 5 5 9:\n",
      ": 4 5 5 9:\n",
      ": 4 5 5 9:\n",
      ": 4 5 5 9:\n",
      ": 4 5 5 9:\n",
      ": 4 5 5 9:\n",
      ": 4 5 5 9:\n",
      ": 4 5 5 9:\n",
      "solution 7: : 1 5 5 6: 2 6 6 7: 3 7 7 8: 4 8 8 9: 5 9 9 10: 6 10 10 11: 7 11 11 12: 8 12 12 13: 9 13 13 14: 10 14 14 15: 11 15 15 16\n",
      "Progress: 1 batches processed. Current accuracy: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution 0: : 1 1 6 12:\n",
      ": 1 1 6 12:\n",
      ": 1 1 6 12:\n",
      ": 1 1 6 12:\n",
      ": 1 1 6 12:\n",
      ": 1 1 6 12:\n",
      ": 1 1 6 12:\n",
      ": 1 1 6 12:\n",
      "solution 1: : 2 8 10 13:\n",
      "\n",
      "Numbers: 2, 2, 8, 8\n",
      "\n",
      "Solution:\n",
      "\n",
      "1. Start with the numbers 2, 2, 8, and 8.\n",
      "\n",
      "2. Group 2 and 8 to get (2 + 8) = 8\n",
      "solution 2: : 5 8 8 9:\n",
      "\n",
      "Numbers: 5, 5, 8, 8\n",
      "\n",
      "Solution:\n",
      "\n",
      "1. Start with the numbers 5, 5, 8, and 8.\n",
      "\n",
      "2. Group 5 and 8 to get (5 + 8) = 10\n",
      "solution 3: : 2 3 7 13:\n",
      "\n",
      "Solution:\n",
      "\n",
      "1. Start with the numbers 2, 2, 7, and 7.\n",
      "\n",
      "2. Group 2 and 7 to get (2 + 7) = 9.\n",
      "\n",
      "3. Multiply 9 by 7\n",
      "solution 4: : 1 6 11 13: 2 6 11 13: 3 6 11 13: 4 6 11 13: 5 6 11 13: 6 6 11 13: 7 6 11 13: 8 6 11 13: 9 6 11 13: 10 6 11 13: 11 6 11 13\n",
      "solution 5: : 1 3 5 6: 1 3 5 6: 1 3 5 6: 1 3 5 6: 1 3 5 6: 1 3 5 6: 1 3 5 6: 1 3 5 6: 1 3 5 6: 1 3 5 6: 1 3 5 6\n",
      "solution 6: : 1 5 7 10:\n",
      "Solution:\n",
      "1. Start with the numbers 1, 1, 5, 7, and 10.\n",
      "2. Group 1 and 1 to get (1 + 1) = 5.\n",
      "3. Multiply 5 by 7 to get\n",
      "solution 7: : 6 6 8 8:\n",
      "\n",
      "1. Start with the numbers 1, 1, 8, and 8.\n",
      "2. Group 1 and 1 to get (1 + 1) = 2.\n",
      "3. Multiply 2 by 8 to get 2 * 8 =\n",
      "Progress: 2 batches processed. Current accuracy: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution 0: : 6 11 12 12:\n",
      ": 6 11 12 12:\n",
      ": 6 11 12 12:\n",
      ": 6 11 12 12:\n",
      ": 6 11 12 12:\n",
      ": 6 11 12 12:\n",
      ": 6 11 12 12:\n",
      ": 6 11 12 12:\n",
      "solution 1: : 4 8 10 10:\n",
      ": 4 8 10 10:\n",
      ": 4 8 10 10:\n",
      ": 4 8 10 10:\n",
      ": 4 8 10 10:\n",
      ": 4 8 10 10:\n",
      ": 4 8 10 10:\n",
      ": 4 8 10 10:\n",
      "solution 2: : 2 6 6 7: 3 6 6 7: 4 6 6 7: 5 6 6 7: 6 6 6 7: 7 6 6 7: 8 6 6 7: 9 6 6 7: 10 6 6 7: 11 6 6 7: 12 6 6 7\n",
      "solution 3: : 1 2 3 12:\n",
      ": 1 2 3 12:\n",
      ": 1 2 3 12:\n",
      ": 1 2 3 12:\n",
      ": 1 2 3 12:\n",
      ": 1 2 3 12:\n",
      ": 1 2 3 12:\n",
      ": 1 2 3 12:\n",
      "solution 4: : 3 6 6 9:\n",
      ": 3 6 6 9:\n",
      ": 3 6 6 9:\n",
      ": 3 6 6 9:\n",
      ": 3 6 6 9:\n",
      ": 3 6 6 9:\n",
      ": 3 6 6 9:\n",
      ": 3 6 6 9:\n",
      "solution 5: : 2 6 6 8: 2 * 4 + 2 * 8 = 24\n",
      ": 2 6 6 8: 2 * 4 + 2 * 8 = 24\n",
      ": 2 6 6 8: 2 * 4 + 2 * 8 = 24\n",
      ": 2 6 6 8: 2\n",
      "solution 6: : 2 2 7 12:\n",
      "\n",
      "Solution:\n",
      "\n",
      "1. Start with the numbers 2, 2, 7, and 12.\n",
      "\n",
      "2. Group 2 and 2 to get (2 + 2) = 6.\n",
      "\n",
      "3. Multiply 6 by 2\n",
      "solution 7: : 2 2 5 6:\n",
      ": 2 2 5 6:\n",
      ": 2 2 5 6:\n",
      ": 2 2 5 6:\n",
      ": 2 2 5 6:\n",
      ": 2 2 5 6:\n",
      ": 2 2 5 6:\n",
      ": 2 2 5 6:\n",
      "Progress: 3 batches processed. Current accuracy: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution 0: : 2 2 3 4:\n",
      ": 2 2 3 4:\n",
      ": 2 2 3 4:\n",
      ": 2 2 3 4:\n",
      ": 2 2 3 4:\n",
      ": 2 2 3 4:\n",
      ": 2 2 3 4:\n",
      ": 2 2 3 4:\n",
      "solution 1: : 2 11 12 13:\n",
      "\n",
      "Solution:\n",
      "\n",
      "1. Start with the numbers 11, 12, 13, and 13.\n",
      "\n",
      "2. Group 11 and 12 to get (11 + 12) = 13.\n",
      "\n",
      "3. Multiply 13 by 13\n",
      "solution 2: : 3 5 10 13:\n",
      "\n",
      "Solution:\n",
      "\n",
      "1. Start with the numbers 1, 1, 8, 8.\n",
      "\n",
      "2. Group 1 and 1 to get (1 + 1) = 2.\n",
      "\n",
      "3. Multiply 2 by 8 to\n",
      "solution 3: : 5 6 11 11:\n",
      "Solution:\n",
      "1. Start with the numbers 5, 6, 11, and 11.\n",
      "2. Group 5 and 11 to get (5 + 11) = 11.\n",
      "3. Multiply 11 by 5 to get 11 *\n",
      "solution 4: : 1 4 6 6: 2 4 6 6: 3 4 6 6: 4 4 6 6: 5 4 6 6: 6 4 6 6: 7 4 6 6: 8 4 6 6: 9 4 6 6: 10 4 6 6: 11 4 6 6\n",
      "solution 5: : 3 10 10 12:\n",
      "\n",
      "Numbers: 1, 1, 8, 8\n",
      "\n",
      "Solution:\n",
      "\n",
      "1. Start with the numbers 1, 1, 8, and 8.\n",
      "\n",
      "2. Group 1 and 1 to get (1 + 1) = 2\n",
      "solution 6: : 5 6 6 9:\n",
      ": 6 7 7 9:\n",
      ": 7 8 8 9:\n",
      ": 8 9 9 9:\n",
      ": 10 10 10 10:\n",
      ": 11 11 11 11:\n",
      ": 12 12 12 12:\n",
      ": 13 13 13 13:\n",
      "solution 7: : 6 7 12 12:\n",
      ": 6 7 12 12:\n",
      ": 6 7 12 12:\n",
      ": 6 7 12 12:\n",
      ": 6 7 12 12:\n",
      ": 6 7 12 12:\n",
      ": 6 7 12 12:\n",
      ": 6 7 12 12:\n",
      "Progress: 4 batches processed. Current accuracy: 0.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(student_model, dataloader, student_tokenizer)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, dataloader, tokenizer, progress_interval)\u001b[0m\n\u001b[0;32m      7\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Generate solutions from the model\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     generated_outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Decode puzzles for evaluation\u001b[39;00m\n\u001b[0;32m     14\u001b[0m puzzles \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mreplace(instructions, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m ids \u001b[38;5;129;01min\u001b[39;00m input_ids]\n",
      "File \u001b[1;32mc:\\Users\\gites\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1914\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1906\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1907\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1908\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   1909\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1910\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1911\u001b[0m     )\n\u001b[0;32m   1913\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 1914\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   1915\u001b[0m         input_ids,\n\u001b[0;32m   1916\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   1917\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mprepared_logits_warper,\n\u001b[0;32m   1918\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   1919\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   1920\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   1921\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   1922\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1923\u001b[0m     )\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   1926\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[0;32m   1927\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1928\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   1929\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[0;32m   1930\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1931\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:2651\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[0;32m   2648\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2650\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2651\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[0;32m   2652\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2653\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2654\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   2655\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2656\u001b[0m )\n\u001b[0;32m   2658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2659\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gites\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gites\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1421\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1413\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1414\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1416\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1419\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1421\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m   1422\u001b[0m     input_ids,\n\u001b[0;32m   1423\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1424\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1425\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1426\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1427\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1428\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1429\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1430\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m   1431\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1432\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1433\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1434\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1435\u001b[0m )\n\u001b[0;32m   1436\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gites\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gites\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1235\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1223\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1224\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m   1225\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1232\u001b[0m         output_attentions,\n\u001b[0;32m   1233\u001b[0m     )\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1235\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m block(\n\u001b[0;32m   1236\u001b[0m         hidden_states,\n\u001b[0;32m   1237\u001b[0m         layer_past\u001b[38;5;241m=\u001b[39mlayer_past,\n\u001b[0;32m   1238\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1239\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask[i],\n\u001b[0;32m   1240\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1241\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m   1242\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1243\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1244\u001b[0m     )\n\u001b[0;32m   1246\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\gites\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gites\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_model(student_model, dataloader, student_tokenizer)\n",
    "print(f\"Model accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
